{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGZ8h+kVpHpAgTTI5Sj72P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avireddi08/AICTE_INTERNSHIP_P1/blob/main/Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHifv_fRSQtH"
      },
      "outputs": [],
      "source": [
        "# Problem Statement: Generate concise medical summaries from lengthy doctor-patient conversations/patient data for faster review.\n",
        "# Why: Concise medical summaries streamline documentation, saving time and improving efficiency for healthcare providers. They allow quick review of key information, aiding faster decision-making and better patient care."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) from transformers import pipeline:\n",
        "\n",
        "* The transformers library is a powerful toolkit built on top of PyTorch and TensorFlow, primarily used for tasks involving pre-trained language models like BERT, GPT-2, etc.\n",
        "* The pipeline function simplifies the process of using these models for common NLP tasks like text summarization, sentiment analysis, translation, and more. It provides a high-level interface to load a pre-trained model and use it directly without worrying about the underlying details.\n",
        "\n",
        "\n",
        "2) from sklearn.feature_extraction.text import TfidfVectorizer:\n",
        "* sklearn (scikit-learn) is a popular Python library for machine learning.\n",
        "* TfidfVectorizer is a tool used for converting text into numerical representations that machine learning algorithms can understand. It does this using a technique called TF-IDF (Term Frequency-Inverse Document Frequency).\n",
        "\n"
      ],
      "metadata": {
        "id": "3yv6gH_JUVd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# Load spaCy model for NER (Named Entity Recognition)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the summarization pipeline (using BART for abstractive summarization)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")'''"
      ],
      "metadata": {
        "id": "7Wf32OFQUYOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def remove_greetings(text):\n",
        "    # List of common greeting words/phrases\n",
        "    greetings = [\n",
        "        r'\\bhello\\b', r'\\bhi\\b', r'\\bhey\\b', r'\\bhow are you\\b',\n",
        "        r'\\bgood morning\\b', r'\\bgood afternoon\\b', r'\\bgood evening\\b',\n",
        "        r'\\bwhat\\'s up\\b', r'\\bhowdy\\b', r'\\bgreetings\\b', r'\\bsalutations\\b'\n",
        "    ]\n",
        "\n",
        "    # Pattern that matches any of the greetings\n",
        "    greetings_pattern = '|'.join(greetings)\n",
        "\n",
        "    # Regex to replace greetings with an empty string\n",
        "    cleaned_text = re.sub(greetings_pattern, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove extra spaces resulting from replacements\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "\n",
        "    return cleaned_text'''"
      ],
      "metadata": {
        "id": "6l9W_M7UUfga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Named Entity Recognition (NER)\n",
        "def extract_medical_entities(text, nlp_model):\n",
        "    doc = nlp_model(text)\n",
        "    entities = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in [\"DISEASE\", \"SYMPTOM\", \"TREATMENT\", \"MEDICATION\"]:\n",
        "            entities.append(ent.text)\n",
        "\n",
        "    return entities'''"
      ],
      "metadata": {
        "id": "r-PRwsFTUjx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) token.dep_: This part accesses the dependency label assigned to the current token. Dependency labels describe the grammatical relationship between words in a sentence, like subject, object, modifier, etc. These labels are determined during dependency parsing.\n",
        "\n",
        "2) ['nsubj', 'dobj', 'ROOT']: This is a list containing specific dependency labels that the code is interested in:\n",
        "\n",
        "* nsubj: Represents the nominal subject of a verb. In simpler terms, it's typically the noun or pronoun that performs the action of the verb.\n",
        "* dobj: Represents the direct object of a verb. It's the noun or pronoun that receives the action of the verb.\n",
        "* ROOT: Represents the main verb or the central element of the sentence that everything else relates to.\n",
        "\n",
        "Example:\n",
        "\n",
        "Let's say you have the sentence \"The cat sat on the mat.\" For the word \"sat,\" the dependency label might be ROOT, the token.text would be \"sat,\" and the token.head.text would also be \"sat\" (since the root is usually its own head).\n",
        "\n",
        "So, the line of code would create the string \"ROOT: sat -> sat\" and add it to the relations list"
      ],
      "metadata": {
        "id": "niCTHgubUmPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Dependency Parsing (to understand context)\n",
        "def dependency_parse(text, nlp_model):\n",
        "    doc = nlp_model(text)\n",
        "    relations = []\n",
        "    for token in doc:\n",
        "        if token.dep_ in ['nsubj', 'dobj', 'ROOT']:\n",
        "            relations.append(f'{token.dep_}: {token.text} -> {token.head.text}')\n",
        "    return relations\n",
        "    # Do not consider this cell #'''"
      ],
      "metadata": {
        "id": "-GdfLu0vUqKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) fit_transform does two things:\n",
        "* It \"fits\" the vectorizer to the text, which means it learns the vocabulary and IDF weights.\n",
        "* It \"transforms\" the text into a matrix where each row represents a document (in this case, just one document - your input text) and each column represents a word. The values in the matrix are the TF-IDF scores.\n",
        "\n",
        "2)    keywords = [word for word, score in zip(vectorizer.get_feature_names_out(), X.sum(axis=0).tolist()[0]) if score > 0]\n",
        "* This line extracts the actual keywords.\n",
        "* It iterates through each word and its corresponding TF-IDF score.\n",
        "* vectorizer.get_feature_names_out(): Retrieves the list of words (features) used in the TF-IDF matrix.\n",
        "* X.sum(axis=0).tolist()[0]: Calculates the sum of TF-IDF scores for each word across all documents (which is just one document here).\n",
        "* zip: Combines the words and their scores.\n",
        "* if score > 0: Only words with a TF-IDF score greater than 0 are considered as keywords. These words are appended to the keywords list."
      ],
      "metadata": {
        "id": "v7yloTD-Uvzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Keyword Extraction using TF-IDF\n",
        "def extract_keywords(text):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform([text])\n",
        "    keywords = [word for word, score in zip(vectorizer.get_feature_names_out(), X.sum(axis=0).tolist()[0]) if score > 0]\n",
        "    return keywords'''"
      ],
      "metadata": {
        "id": "ckWnYloVUtB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Function to generate an insightful summary\n",
        "def generate_summary_for_processed_data(text, summarizer):\n",
        "    # Use the summarizer to generate the summary\n",
        "    # model will use deterministic approach to find the summary rather than using random sampling.\n",
        "    summary_data = summarizer(text, max_length=120, min_length=50, do_sample=False)\n",
        "    return summary_data[0]['summary_text']'''"
      ],
      "metadata": {
        "id": "Nb-tGcPnU0o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Function to generate an insightful summary for processed conversation.\n",
        "def generate_summary_for_processed_conversation(text, summarizer):\n",
        "    # Use the summarizer to generate the summary\n",
        "    # model will use deterministic approach to find the summary rather than using random sampling.\n",
        "    summary_conversation = summarizer(text, max_length=350, min_length=150, do_sample=False)\n",
        "    return summary_conversation[0]['summary_text']'''"
      ],
      "metadata": {
        "id": "rcxpg4BbU23X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Organize extracted content into sections\n",
        "def create_medical_summary_for_processed_data(text, nlp_model, summarizer):\n",
        "\n",
        "    entities = extract_medical_entities(text, nlp_model)\n",
        "    # relations = dependency_parse(text, nlp_model)\n",
        "    keywords = extract_keywords(text)\n",
        "\n",
        "    summary = generate_summary_for_processed_data(text, summarizer)\n",
        "\n",
        "    # Structuring the summary\n",
        "    summary_dict = {\n",
        "        \"Entities\": entities,\n",
        "        # \"Relations\": relations,\n",
        "        \"Keywords\": keywords,\n",
        "        \"Summary\": summary\n",
        "    }\n",
        "    return summary_dict'''"
      ],
      "metadata": {
        "id": "Cqrs1PPhU5t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Organize extracted content into sections\n",
        "def create_medical_summary_for_processed_conversation(text, nlp_model, summarizer):\n",
        "\n",
        "    entities = extract_medical_entities(text, nlp_model)\n",
        "    # relations = dependency_parse(text, nlp_model)\n",
        "    keywords = extract_keywords(text)\n",
        "\n",
        "    summary = generate_summary_for_processed_conversation(text, summarizer)\n",
        "\n",
        "    # Structuring the summary\n",
        "    summary_dict = {\n",
        "        \"Entities\": entities,\n",
        "        # \"Relations\": relations,\n",
        "        \"Keywords\": keywords,\n",
        "        \"Summary\": summary\n",
        "    }\n",
        "    return summary_dict'''"
      ],
      "metadata": {
        "id": "40i0rCMeU8Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Summarization using Abstractive Summarization model\n",
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Using a popular summarization model (e.g., BART)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Process 10 rows in the processed_conversation\n",
        "for index, row in sampled_df.head(10).iterrows():  # Iterating through 10 rows\n",
        "    conversation_text = row['processed_conversation']  # Extract the processed_conversation text\n",
        "    conversation_text = remove_greetings(conversation_text)  # Remove greetings\n",
        "    summary = create_medical_summary_for_processed_conversation(conversation_text, nlp, summarizer)  # Generate summary for each row\n",
        "\n",
        "    # Output the structured summary for the current row\n",
        "    print(f\"Summary for row {index}:\")\n",
        "    print(f\"Entities: {summary['Entities']}\")\n",
        "    # print(f\"Relations: {summary['Relations']}\")\n",
        "    print(f\"Keywords: {summary['Keywords']}\")\n",
        "    print(f\"Summary: {summary['Summary']}\")\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")'''"
      ],
      "metadata": {
        "id": "jyj_ea1GU-n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Summarization using Abstractive Summarization model\n",
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Using a popular summarization model (e.g., BART)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Process 10 rows in the processed_conversation\n",
        "for index, row in sampled_df.head(10).iterrows():  # Iterating through 10 rows\n",
        "    conversation_text = row['processed_conversation']  # Extract the processed_conversation text\n",
        "    conversation_text = remove_greetings(conversation_text)  # Remove greetings\n",
        "    summary = create_medical_summary_for_processed_conversation(conversation_text, nlp, summarizer)  # Generate summary for each row\n",
        "\n",
        "    # Output the structured summary for the current row\n",
        "    print(f\"Summary for row {index}:\")\n",
        "    print(f\"Entities: {summary['Entities']}\")\n",
        "    # print(f\"Relations: {summary['Relations']}\")\n",
        "    print(f\"Keywords: {summary['Keywords']}\")\n",
        "    print(f\"Summary: {summary['Summary']}\")\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")'''"
      ],
      "metadata": {
        "id": "Su6H8w_MVBZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import spacy\n",
        "import re\n",
        "import streamlit as st\n",
        "\n",
        "# Load spaCy model for NER\n",
        "@st.cache_resource\n",
        "def load_spacy_model():\n",
        "    return spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nlp = load_spacy_model()\n",
        "\n",
        "# Load summarizer pipeline\n",
        "@st.cache_resource\n",
        "def load_summarizer():\n",
        "    return pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
        "\n",
        "summarizer = load_summarizer()\n",
        "\n",
        "def remove_greetings(text):\n",
        "    greetings = [\n",
        "        r'\\bhello\\b', r'\\bhi\\b', r'\\bhey\\b', r'\\bhow are you\\b',\n",
        "        r'\\bgood morning\\b', r'\\bgood afternoon\\b', r'\\bgood evening\\b',\n",
        "        r'\\bwhat\\'s up\\b', r'\\bhowdy\\b', r'\\bgreetings\\b', r'\\bsalutations\\b'\n",
        "    ]\n",
        "    greetings_pattern = '|'.join(greetings)\n",
        "    cleaned_text = re.sub(greetings_pattern, '', text, flags=re.IGNORECASE)\n",
        "    return re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "\n",
        "def extract_medical_entities(text, nlp_model):\n",
        "    doc = nlp_model(text)\n",
        "    return [ent.text for ent in doc.ents if ent.label_ in [\"DISEASE\", \"SYMPTOM\", \"TREATMENT\", \"MEDICATION\"]]\n",
        "\n",
        "def extract_keywords(text):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform([text])\n",
        "    return [word for word, score in zip(vectorizer.get_feature_names_out(), X.toarray()[0]) if score > 0]\n",
        "\n",
        "def generate_summary(text, max_len, min_len):\n",
        "    summary_data = summarizer(text, max_length=max_len, min_length=min_len, do_sample=False)\n",
        "    return summary_data[0]['summary_text']\n",
        "\n",
        "def create_summary(text, nlp_model, max_len, min_len):\n",
        "    entities = extract_medical_entities(text, nlp_model)\n",
        "    keywords = extract_keywords(text)\n",
        "    summary = generate_summary(text, max_len, min_len)\n",
        "    return {\n",
        "        \"Entities\": entities,\n",
        "        \"Keywords\": keywords,\n",
        "        \"Summary\": summary\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    st.title(\"Medical Text Analysis Tool\")\n",
        "    st.write(\"This tool extracts medical entities, keywords, and summarizes the input text.\")\n",
        "\n",
        "    user_input = st.text_area(\"Enter the medical text here:\")\n",
        "    summary_type = st.radio(\"Select Summary Type\", (\"Data\", \"Conversation\"))\n",
        "\n",
        "    if st.button(\"Process\"):\n",
        "        if user_input.strip():\n",
        "            processed_text = remove_greetings(user_input)\n",
        "            max_len, min_len = (120, 50) if summary_type == \"Data\" else (350, 150)\n",
        "            summary = create_summary(processed_text, nlp, max_len, min_len)\n",
        "\n",
        "            st.subheader(\"Extracted Medical Entities\")\n",
        "            st.write(\", \".join(summary['Entities']) if summary['Entities'] else \"No entities found.\")\n",
        "\n",
        "            st.subheader(\"Extracted Keywords\")\n",
        "            st.write(\", \".join(summary['Keywords']) if summary['Keywords'] else \"No keywords found.\")\n",
        "\n",
        "            st.subheader(\"Text Summary\")\n",
        "            st.write(summary['Summary'])\n",
        "        else:\n",
        "            st.warning(\"Please enter some text to process.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "oAToofNzVG7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Install necessary libraries in one command\n",
        "!pip install -q streamlit spacy transformers scikit-learn localtunnel\n",
        "\n",
        "# Download the spaCy model for English\n",
        "!python -m spacy download en_core_web_sm'''"
      ],
      "metadata": {
        "id": "Dug2iqJXVK9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -o - ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "VcYuBjNyVNF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run MedicalTextAnalysis.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "i_Ivbh63VO0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of data lengths\n",
        "sampled_df['data_length'] = sampled_df['processed_data'].apply(lambda x: len(x.split()))\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(sampled_df['data_length'], kde=True, bins=30)\n",
        "plt.title('Distribution of Data Lengths')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4kpxSQNQW_F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of conversation lengths\n",
        "sampled_df['conversation_length'] = sampled_df['processed_conversation'].apply(lambda x: len(x.split()))\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(sampled_df['conversation_length'], kde=True, bins=30)\n",
        "plt.title('Distribution of Conversation Lengths')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tkwwf5aAXBXm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}